# üîç –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú –ò –ü–õ–ê–ù –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô

**–î–∞—Ç–∞:** 8 —è–Ω–≤–∞—Ä—è 2026  
**–°—Ç–∞—Ç—É—Å:** üìã –ü–õ–ê–ù –ö –û–ë–°–£–ñ–î–ï–ù–ò–Æ (—Ç—Ä–µ–±—É–µ—Ç –æ–¥–æ–±—Ä–µ–Ω–∏—è)

---

## üéØ –ü–†–û–ë–õ–ï–ú–ê 1: –ü–æ–≤—Ç–æ—Ä –∞—É–¥–∏–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 —Å–µ–∫—É–Ω–¥–∞—Ö

### –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π –ª–æ–≥–∏–∫–∏:

#### üìç **–ú–µ—Å—Ç–æ –ø—Ä–æ–±–ª–µ–º—ã:** `app/services/faceless_engine.py`

**–°—Ç—Ä–æ–∫–∞ 996-1011:**
```python
cmd = [
    FFMPEG_PATH, "-y",
    "-i", video_path,           # –í—Ö–æ–¥ 0: –≤–∏–¥–µ–æ (—Å–∫–ª–µ–µ–Ω–Ω—ã–µ –∫–ª–∏–ø—ã)
    "-i", job.audio_path,        # –í—Ö–æ–¥ 1: –∞—É–¥–∏–æ (TTS)
    "-filter_complex", f"[0:v]{filter_str}[vout]",
    "-map", "[vout]",
    "-map", "1:a",               # –ë–µ—Ä—ë–º –∞—É–¥–∏–æ –∏–∑ –≤—Ö–æ–¥–∞ 1
    "-c:v", "libx264",
    "-preset", "fast",
    "-crf", "18",
    "-c:a", "aac",
    "-b:a", "192k",
    "-t", str(job.audio_duration),  # ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê –ó–î–ï–°–¨!
    "-shortest",                     # ‚ö†Ô∏è –ò –ó–î–ï–°–¨!
    output_path
]
```

### üêõ **–ü—Ä–∏—á–∏–Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã:**

#### –°—Ü–µ–Ω–∞—Ä–∏–π –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è:
```
1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ (TTS):
   –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 57.3 —Å–µ–∫—É–Ω–¥—ã

2. –†–∞—Å—á—ë—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤:
   12 —Å–µ–≥–º–µ–Ω—Ç–æ–≤ √ó 5 —Å–µ–∫—É–Ω–¥ = 60 —Å–µ–∫—É–Ω–¥

3. Ken Burns –∞–Ω–∏–º–∞—Ü–∏—è:
   –°–æ–∑–¥–∞—ë—Ç 12 –∫–ª–∏–ø–æ–≤ √ó 5 —Å–µ–∫ = 60 —Å–µ–∫—É–Ω–¥

4. –°–∫–ª–µ–π–∫–∞ –∫–ª–∏–ø–æ–≤:
   –ò—Ç–æ–≥–æ–≤–æ–µ –≤–∏–¥–µ–æ: 60 —Å–µ–∫—É–Ω–¥

5. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä FFmpeg:
   -t 57.3  ‚Üí –û–±—Ä–µ–∑–∞–µ—Ç –¥–æ 57.3 —Å–µ–∫
   -shortest ‚Üí –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
   
   –ù–û! –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–ª–∏–ø —É–∂–µ –Ω–∞—á–∞–ª—Å—è (55-60 —Å–µ–∫),
   –∞ –∞—É–¥–∏–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ 57.3 —Å–µ–∫,
   FFmpeg –º–æ–∂–µ—Ç "–∑–∞—Ü–∏–∫–ª–∏—Ç—å" –∞—É–¥–∏–æ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è!
```

### üîç **–ì–ª—É–±–∏–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞:**

**–í `_calculate_segment_durations()` (—Å—Ç—Ä–æ–∫–∞ 937-962):**
```python
def _calculate_segment_durations(
    self,
    segments: List[Dict[str, Any]],
    total_audio_duration: float
) -> List[float]:
    """Calculate segment durations based on audio timing."""
    num_segments = len(segments)
    avg_duration = total_audio_duration / num_segments
    
    # ... –¥–∞–ª—å—à–µ –∫–æ–¥ ...
    
    durations = [max(d, min_duration) for d in durations]
    return durations
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –°—É–º–º–∞ `durations` –º–æ–∂–µ—Ç –±—ã—Ç—å **–±–æ–ª—å—à–µ** `total_audio_duration`!
- –ù–∞–ø—Ä–∏–º–µ—Ä: –∞—É–¥–∏–æ 57.3—Å, –Ω–æ –≤–∏–¥–µ–æ 60—Å
- –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2.7 —Å–µ–∫—É–Ω–¥—ã –≤–∏–¥–µ–æ = **–±–µ–∑ –∞—É–¥–∏–æ** –∏–ª–∏ **–∑–∞—Ü–∏–∫–ª–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ**

---

## üéØ –ü–†–û–ë–õ–ï–ú–ê 2: –•–∞–æ—Ç–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ (–Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)

### –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–π –ª–æ–≥–∏–∫–∏:

#### üìç **–ú–µ—Å—Ç–æ –ø—Ä–æ–±–ª–µ–º—ã:** `app/services/agents/visual_director.py`

**–¢–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å (—Å—Ç—Ä–æ–∫–∞ 286-448):**
```python
async def segment_story(self, narrative, topic, style, ...):
    # 1. –ü–æ–ª—É—á–∞–µ—Ç –≤–µ—Å—å narrative —Ü–µ–ª–∏–∫–æ–º ‚úÖ
    
    # 2. –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç ONE API call –≤ GPT-4o-mini
    payload = {
        "model": self.model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}  # –í–µ—Å—å narrative
        ],
        ...
    }
    
    # 3. GPT –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 12 —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏
    
    # 4. Post-processing:
    segments = self._clean_repetitive_phrases(segments)
    segments = self.deduplicate_prompts(segments)
    
    return SegmentationResult(segments=segments, ...)
```

### üêõ **–ü—Ä–æ–±–ª–µ–º—ã:**

#### **–ü—Ä–æ–±–ª–µ–º–∞ 2.1: –ù–µ—Ç —è–≤–Ω–æ–≥–æ "global_scene_context"**

**–¢–µ–∫—É—â–∏–π prompt (—Å—Ç—Ä–æ–∫–∞ 346-396):**
```python
user_prompt = f"""Divide this narrative into EXACTLY {segment_count} segments...

NARRATIVE:
{narrative}

TOPIC: {topic}
STYLE: {style.value.upper()}

CHARACTER CONSISTENCY (ABSOLUTELY CRITICAL!):
If a character appears in multiple segments, use IDENTICAL description...
```

**–ß–¢–û –ù–ï –¢–ê–ö:**
- ‚úÖ GPT –≤–∏–¥–∏—Ç –≤–µ—Å—å narrative
- ‚úÖ –ï—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–æ character consistency
- ‚ùå –ù–ï–¢ explicit "global_scene_context" –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
- ‚ùå –ù–µ—Ç pre-analysis: "–ö–∞–∫–∞—è —ç–ø–æ—Ö–∞? –ö–∞–∫–æ–π —Å—Ç–∏–ª—å –æ—Å–≤–µ—â–µ–Ω–∏—è? –ö–∞–∫–∞—è –ø–∞–ª–∏—Ç—Ä–∞?"

**–†–ï–ó–£–õ–¨–¢–ê–¢:**
- –°–µ–≥–º–µ–Ω—Ç 1: "Medieval castle, dark lighting, foggy atmosphere"
- –°–µ–≥–º–µ–Ω—Ç 5: "Castle interior, bright daylight, clear sky"
- ‚ùå –ù–ï–°–û–ì–õ–ê–°–û–í–ê–ù–ù–û–°–¢–¨! –î–µ–Ω—å —Å—Ç–∞–ª –Ω–æ—á—å—é? –¢—É–º–∞–Ω –∏—Å—á–µ–∑?

#### **–ü—Ä–æ–±–ª–µ–º–∞ 2.2: –ù–µ—Ç "Storyboard" —ç—Ç–∞–ø–∞**

**–¢–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å:**
```
1. GPT –ø–æ–ª—É—á–∞–µ—Ç narrative
2. GPT —Å—Ä–∞–∑—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 12 –ø—Ä–æ–º–ø—Ç–æ–≤
3. –ì–æ—Ç–æ–≤–æ
```

**–ß–¢–û –ù–ï –¢–ê–ö:**
- –ù–µ—Ç —ç—Ç–∞–ø–∞ **–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è**: "–ö–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã?"
- –ù–µ—Ç —ç—Ç–∞–ø–∞ **–∞–Ω–∞–ª–∏–∑–∞**: "–ö–∞–∫–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏? –ö–∞–∫–∏–µ –ª–æ–∫–∞—Ü–∏–∏?"
- –ù–µ—Ç —ç—Ç–∞–ø–∞ **—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è**: "–ö–∞–¥—Ä 5 –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–ª–µ–¥—É–µ—Ç –∏–∑ –∫–∞–¥—Ä–∞ 4?"

**–ò–î–ï–ê–õ–¨–ù–´–ô –ü–†–û–¶–ï–°–° (–∫–∞–∫ –≤ autoshorts.ai/Hollywood):**
```
1. ANALYSIS: –ê–Ω–∞–ª–∏–∑ narrative ‚Üí –ò–∑–≤–ª–µ—á—å:
   - –ü–µ—Ä—Å–æ–Ω–∞–∂–∏ (—Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º)
   - –õ–æ–∫–∞—Ü–∏–∏ (—Å –∞—Ç–º–æ—Å—Ñ–µ—Ä–æ–π)
   - –í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ (—ç–ø–æ—Ö–∞, –≤—Ä–µ–º—è —Å—É—Ç–æ–∫)
   - –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞
   - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∞—Ä–∫–∞

2. STORYBOARD: –°–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω:
   - –ö–∞–¥—Ä 1: [–ü–µ—Ä—Å–æ–Ω–∞–∂ X] –≤ [–õ–æ–∫–∞—Ü–∏—è Y] - [–î–µ–π—Å—Ç–≤–∏–µ] - [–≠–º–æ—Ü–∏—è]
   - –ö–∞–¥—Ä 2: [–¢–æ—Ç –∂–µ –ø–µ—Ä—Å–æ–Ω–∞–∂] –≤ [–¢–∞ –∂–µ –ª–æ–∫–∞—Ü–∏—è] - [–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è]
   - ...
   - –ü—Ä–æ–≤–µ—Ä–∫–∞: –ï—Å—Ç—å –ª–∏ –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏?

3. PROMPTS: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å storyboard ‚Üí —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã:
   - –í—Å–µ –ø—Ä–æ–º–ø—Ç—ã –Ω–∞—Å–ª–µ–¥—É—é—Ç global_scene_context
   - –ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –û–î–ò–ù–ê–ö–û–í–û –≤–æ –≤—Å–µ—Ö –∫–∞–¥—Ä–∞—Ö
   - –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞ (–µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –¥–µ–Ω—å‚Üí–Ω–æ—á—å)
```

#### **–ü—Ä–æ–±–ª–µ–º–∞ 2.3: –ù–µ—Ç frame-to-frame consistency check**

**–ü–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –ù–ï–¢ –ø—Ä–æ–≤–µ—Ä–∫–∏:**
```python
# –ù–ï–¢ –≠–¢–û–ì–û:
def check_frame_consistency(segment_n, segment_n_plus_1):
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏.
    
    –í–æ–ø—Ä–æ—Å—ã:
    - –ü–µ—Ä—Å–æ–Ω–∞–∂ —Ç–æ—Ç –∂–µ?
    - –õ–æ–∫–∞—Ü–∏—è —Ç–∞ –∂–µ –∏–ª–∏ –ª–æ–≥–∏—á–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥?
    - –û—Å–≤–µ—â–µ–Ω–∏–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ?
    - –ù–µ—Ç –≤–Ω–µ–∑–∞–ø–Ω—ã—Ö —Å–∫–∞—á–∫–æ–≤ (–¥–µ–Ω—å‚Üí–Ω–æ—á—å –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è)?
    """
    pass
```

---

## üõ†Ô∏è –ü–õ–ê–ù –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô

### ‚úÖ **–≠–¢–ê–ü 1: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏**

#### –§–∞–π–ª: `app/services/faceless_engine.py`

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 1.1:** –°—Ç—Ä–æ–∫–∞ 937-962 - `_calculate_segment_durations()`

**–ë–´–õ–û:**
```python
def _calculate_segment_durations(self, segments, total_audio_duration):
    num_segments = len(segments)
    avg_duration = total_audio_duration / num_segments
    # ... —Ä–∞—Å—á—ë—Ç ...
    durations = [max(d, min_duration) for d in durations]
    return durations
```

**–°–¢–ê–ù–ï–¢:**
```python
def _calculate_segment_durations(self, segments, total_audio_duration):
    """
    Calculate segment durations that EXACTLY match audio duration.
    
    CRITICAL FIX: Ensures sum(durations) == total_audio_duration
    to prevent audio repetition in last frames.
    """
    num_segments = len(segments)
    min_duration = 2.0  # –ú–∏–Ω–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç
    
    # Initial equal distribution
    base_duration = total_audio_duration / num_segments
    durations = [base_duration] * num_segments
    
    # Apply min_duration constraint
    for i in range(num_segments):
        if durations[i] < min_duration:
            shortage = min_duration - durations[i]
            durations[i] = min_duration
            
            # Redistribute shortage across other segments
            remaining_segments = num_segments - i - 1
            if remaining_segments > 0:
                per_segment_reduction = shortage / remaining_segments
                for j in range(i + 1, num_segments):
                    durations[j] = max(min_duration, durations[j] - per_segment_reduction)
    
    # CRITICAL: Force exact match to audio duration
    current_total = sum(durations)
    if abs(current_total - total_audio_duration) > 0.1:  # 100ms tolerance
        # Scale all durations proportionally
        scale_factor = total_audio_duration / current_total
        durations = [d * scale_factor for d in durations]
        
        logger.info(f"[DURATION_FIX] Scaled durations to match audio: {current_total:.2f}s ‚Üí {total_audio_duration:.2f}s")
    
    # Verify
    final_total = sum(durations)
    logger.info(f"[DURATION_CHECK] Audio: {total_audio_duration:.2f}s, Video segments: {final_total:.2f}s, Diff: {abs(final_total - total_audio_duration):.3f}s")
    
    return durations
```

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 1.2:** –°—Ç—Ä–æ–∫–∞ 996-1011 - `_render_final_video()`

**–ë–´–õ–û:**
```python
cmd = [
    FFMPEG_PATH, "-y",
    "-i", video_path,
    "-i", job.audio_path,
    "-filter_complex", f"[0:v]{filter_str}[vout]",
    "-map", "[vout]",
    "-map", "1:a",
    "-c:v", "libx264",
    "-preset", "fast",
    "-crf", "18",
    "-c:a", "aac",
    "-b:a", "192k",
    "-t", str(job.audio_duration),  # ‚ö†Ô∏è –ú–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
    "-shortest",                     # ‚ö†Ô∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç —Å -t
    output_path
]
```

**–°–¢–ê–ù–ï–¢:**
```python
# CRITICAL FIX: Remove conflicting -t and -shortest flags
# Instead, ensure video and audio are EXACTLY same duration before FFmpeg
cmd = [
    FFMPEG_PATH, "-y",
    "-i", video_path,
    "-i", job.audio_path,
    "-filter_complex", f"[0:v]{filter_str}[vout]",
    "-map", "[vout]",
    "-map", "1:a",
    "-c:v", "libx264",
    "-preset", "fast",
    "-crf", "18",
    "-c:a", "aac",
    "-b:a", "192k",
    # ‚úÖ REMOVED: "-t", str(job.audio_duration),
    # ‚úÖ REMOVED: "-shortest",
    # Instead, video segments are pre-calculated to match audio exactly
    output_path
]
```

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 1.3:** –°—Ç—Ä–æ–∫–∞ 846-850 - Pre-verification –ø–µ—Ä–µ–¥ concatenation

**–î–û–ë–ê–í–ò–¢–¨:**
```python
# Concatenate animated clips (only if we have clips)
concat_video_path = str(job_dir / "concat_video.mp4")
if animated_clips:
    # CRITICAL: Verify total clip duration matches audio
    total_clip_duration = sum(clip.duration for clip in animated_clips)
    audio_duration = job.audio_duration
    
    if abs(total_clip_duration - audio_duration) > 0.5:  # 500ms tolerance
        logger.warning(f"[DURATION_MISMATCH] Clips: {total_clip_duration:.2f}s, Audio: {audio_duration:.2f}s")
        logger.warning(f"[DURATION_MISMATCH] Adjusting last clip to match audio exactly")
        
        # Adjust last clip duration
        diff = audio_duration - (total_clip_duration - animated_clips[-1].duration)
        animated_clips[-1].duration = max(2.0, diff)  # Min 2 seconds
    
    await self.ken_burns.concatenate_clips(animated_clips, concat_video_path)
```

---

### ‚úÖ **–≠–¢–ê–ü 2: Context-Aware Visual Generation**

#### –§–∞–π–ª: `app/services/agents/visual_director.py`

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 2.1:** –î–æ–±–∞–≤–∏—Ç—å dataclass –¥–ª—è Global Scene Context

**–î–û–ë–ê–í–ò–¢–¨ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ (–ø–æ—Å–ª–µ imports):**
```python
@dataclass
class GlobalSceneContext:
    """
    Global visual context for the entire video.
    Ensures all frames share consistent visual language.
    """
    # Core identifiers
    topic: str
    era: str  # "medieval", "modern", "futuristic", etc.
    time_of_day: str  # "morning", "noon", "evening", "night"
    season: str  # "spring", "summer", "fall", "winter"
    
    # Visual style
    color_palette: List[str]  # ["dark blue", "gold", "crimson"]
    lighting_style: str  # "golden hour", "dramatic shadows", "soft diffused"
    atmosphere: str  # "mystical", "gritty", "serene", "tense"
    weather: str  # "clear", "foggy", "rainy", "snowy"
    
    # Characters (consistent descriptions)
    main_characters: List[Dict[str, str]]  # [{"id": "hero", "description": "Man, 35, brown hair, ..."}]
    
    # Locations
    primary_location: str  # "medieval castle"
    secondary_locations: List[str]  # ["throne room", "courtyard", "battlefield"]
    
    # Technical
    art_style: str  # "photorealistic", "anime", etc.
    camera_style: str  # "cinematic", "documentary", "action"
    
    def to_prompt_prefix(self) -> str:
        """Generate consistent prefix for all prompts."""
        return f"{self.era} era, {self.time_of_day} {self.lighting_style}, {self.atmosphere} atmosphere, {self.weather} weather"
```

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 2.2:** –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ - `_analyze_narrative_context()`

**–î–û–ë–ê–í–ò–¢–¨ –≤ –∫–ª–∞—Å—Å VisualDirector:**
```python
async def _analyze_narrative_context(
    self,
    narrative: str,
    topic: str,
    style: ScriptStyle,
    art_style: str
) -> GlobalSceneContext:
    """
    PHASE 1: Analyze narrative to extract global visual context.
    
    This ensures all frames share the same:
    - Era/period
    - Lighting style
    - Color palette
    - Character descriptions
    - Atmospheric mood
    
    Example:
    Input: "–ß–∏–Ω–≥–∏—Å—Ö–∞–Ω —Ä–æ–¥–∏–ª—Å—è –≤ —Å—Ç–µ–ø–∏..."
    Output: GlobalSceneContext(
        era="13th century Mongol Empire",
        time_of_day="golden hour",
        lighting_style="dramatic sunset lighting",
        atmosphere="epic historical",
        main_characters=[{"id": "genghis", "description": "Man, 35, mongolian warrior, ..."}],
        ...
    )
    """
    if not self.api_key:
        # Fallback context
        return self._create_fallback_context(topic, style, art_style)
    
    system_prompt = """You are a VISUAL CONTEXT ANALYZER for video production.

Your task: Analyze a narrative and extract GLOBAL VISUAL CONTEXT that will be applied to ALL frames.

Output JSON with these fields:
{
  "era": "Historical period (e.g., '13th century', 'modern day', 'futuristic 2050')",
  "time_of_day": "morning/noon/evening/night",
  "season": "spring/summer/fall/winter",
  "color_palette": ["color1", "color2", "color3"],
  "lighting_style": "Consistent lighting (e.g., 'golden hour', 'dramatic shadows')",
  "atmosphere": "Overall mood (e.g., 'epic', 'mysterious', 'serene')",
  "weather": "clear/foggy/rainy/snowy",
  "main_characters": [
    {
      "id": "character1",
      "description": "Detailed physical description (e.g., 'Man, 35, brown hair, beard, traditional clothes')"
    }
  ],
  "primary_location": "Main setting",
  "secondary_locations": ["location1", "location2"],
  "camera_style": "cinematic/documentary/action"
}

CRITICAL: This context will be applied to ALL 12 frames, so it must be:
- Consistent (no contradictions)
- Detailed (enough to generate identical visuals)
- Era-appropriate (lighting, colors match the period)
"""

    user_prompt = f"""Analyze this narrative and extract global visual context:

NARRATIVE:
{narrative}

TOPIC: {topic}
STYLE: {style.value}
ART STYLE: {art_style}

Return ONLY valid JSON with the visual context."""

    try:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.3,  # Low temperature for consistency
            "max_tokens": 800,
            "response_format": {"type": "json_object"}
        }
        
        response = await self.client.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code != 200:
            logger.error(f"[CONTEXT_ANALYSIS] API error: {response.status_code}")
            return self._create_fallback_context(topic, style, art_style)
        
        data = response.json()
        content = json.loads(data["choices"][0]["message"]["content"])
        
        # Convert to GlobalSceneContext
        context = GlobalSceneContext(
            topic=topic,
            era=content.get("era", "modern"),
            time_of_day=content.get("time_of_day", "day"),
            season=content.get("season", "summer"),
            color_palette=content.get("color_palette", ["neutral"]),
            lighting_style=content.get("lighting_style", "natural"),
            atmosphere=content.get("atmosphere", "neutral"),
            weather=content.get("weather", "clear"),
            main_characters=content.get("main_characters", []),
            primary_location=content.get("primary_location", topic),
            secondary_locations=content.get("secondary_locations", []),
            art_style=art_style,
            camera_style=content.get("camera_style", "cinematic")
        )
        
        logger.info(f"[CONTEXT_ANALYSIS] Extracted context: {context.era}, {context.atmosphere}, {len(context.main_characters)} characters")
        return context
        
    except Exception as e:
        logger.error(f"[CONTEXT_ANALYSIS] Failed: {e}")
        return self._create_fallback_context(topic, style, art_style)
```

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 2.3:** –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ - `_create_storyboard()`

**–î–û–ë–ê–í–ò–¢–¨:**
```python
async def _create_storyboard(
    self,
    narrative: str,
    segments: List[VisualSegment],
    global_context: GlobalSceneContext
) -> List[VisualSegment]:
    """
    PHASE 2: Create visual storyboard with frame-to-frame consistency.
    
    Takes raw segments and enriches them with:
    - Global context inheritance
    - Character consistency
    - Location transitions
    - Logical flow between frames
    
    Example:
    Frame 4: "Hero in castle courtyard, preparing for battle"
    Frame 5: "Hero on horseback, riding towards battlefield"
    ‚úÖ LOGICAL: Courtyard ‚Üí Battlefield transition makes sense
    
    Frame 4: "Hero in castle, daylight"
    Frame 5: "Hero in cave, nighttime"
    ‚ùå ILLOGICAL: How did he teleport to a cave at night?
    """
    if not self.api_key:
        # Apply fallback context
        return self._apply_fallback_storyboard(segments, global_context)
    
    # Build character map for easy reference
    character_map = {char["id"]: char["description"] for char in global_context.main_characters}
    
    system_prompt = f"""You are a STORYBOARD ARTIST creating visual shot list.

GLOBAL CONTEXT (MUST be applied to ALL frames):
Era: {global_context.era}
Time: {global_context.time_of_day}
Lighting: {global_context.lighting_style}
Atmosphere: {global_context.atmosphere}
Weather: {global_context.weather}
Color Palette: {', '.join(global_context.color_palette)}
Primary Location: {global_context.primary_location}

CHARACTERS (use EXACT descriptions):
{json.dumps(character_map, indent=2)}

Your task: For each segment, create a detailed SHOT DESCRIPTION that:
1. Inherits global context (era, lighting, atmosphere)
2. Uses IDENTICAL character descriptions if character appears
3. Ensures logical transitions between frames
4. Specifies camera angle and shot type

Output JSON array with 12 shots:
[
  {{
    "index": 0,
    "shot_type": "wide shot/close-up/medium shot",
    "subject": "EXACT character description or object",
    "action": "What's happening",
    "location": "Specific location from global context",
    "camera_angle": "low angle/high angle/eye level",
    "transition_from_previous": "How this connects to previous frame (null for frame 0)"
  }},
  ...
]"""

    # Prepare segment texts for context
    segment_texts = [f"Segment {i}: {seg.text}" for i, seg in enumerate(segments)]
    
    user_prompt = f"""Create storyboard for these narrative segments:

{chr(10).join(segment_texts)}

CRITICAL RULES:
1. ALL frames must share: {global_context.era}, {global_context.time_of_day}, {global_context.lighting_style}
2. Character descriptions MUST be IDENTICAL across frames
3. Each frame must logically follow the previous (no teleportation!)
4. Mix shot types for variety (wide, medium, close-up)

Return valid JSON array with 12 detailed shots."""

    try:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.5,
            "max_tokens": 3000,
            "response_format": {"type": "json_object"}
        }
        
        response = await self.client.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code != 200:
            logger.error(f"[STORYBOARD] API error: {response.status_code}")
            return self._apply_fallback_storyboard(segments, global_context)
        
        data = response.json()
        content = json.loads(data["choices"][0]["message"]["content"])
        shots = content.get("shots", content.get("storyboard", []))
        
        # Enrich segments with storyboard data
        for i, seg in enumerate(segments):
            if i < len(shots):
                shot = shots[i]
                # Store storyboard info in segment for prompt generation
                seg.visual_keywords.extend([
                    shot.get("subject", ""),
                    shot.get("location", ""),
                    shot.get("shot_type", "")
                ])
                seg.camera_direction = shot.get("camera_angle", "eye level")
        
        logger.info(f"[STORYBOARD] Created {len(shots)} shots with logical transitions")
        return segments
        
    except Exception as e:
        logger.error(f"[STORYBOARD] Failed: {e}")
        return self._apply_fallback_storyboard(segments, global_context)
```

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 2.4:** –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å `segment_story()` - –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–∑—ã

**–ò–ó–ú–ï–ù–ò–¢–¨ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥:**
```python
async def segment_story(
    self,
    narrative: str,
    topic: str,
    style: ScriptStyle = ScriptStyle.DOCUMENTARY,
    language: str = "ru",
    duration_seconds: int = 60,
    art_style: str = "photorealism"
) -> SegmentationResult:
    """
    Context-Aware Visual Generation (3-phase process):
    
    PHASE 1: CONTEXT ANALYSIS
      - Analyze narrative to extract global visual context
      - Determine era, lighting, atmosphere, characters, locations
      
    PHASE 2: STORYBOARD CREATION
      - Create visual shot list with frame-to-frame consistency
      - Ensure logical transitions between frames
      
    PHASE 3: PROMPT GENERATION
      - Convert storyboard to technical Nano Banana prompts
      - Apply global context to every prompt
      - Deduplicate similar prompts
    """
    segment_count = get_segment_count(duration_seconds)
    logger.info(f"[VISUAL_DIRECTOR] üé¨ CONTEXT-AWARE GENERATION: {segment_count} segments")
    logger.info(f"[VISUAL_DIRECTOR] Topic: {topic}, Style: {style.value}, Art: {art_style}")
    
    # =================================================================
    # PHASE 1: CONTEXT ANALYSIS
    # =================================================================
    logger.info(f"[PHASE_1] üîç Analyzing narrative for global visual context...")
    global_context = await self._analyze_narrative_context(
        narrative, topic, style, art_style
    )
    logger.info(f"[PHASE_1] ‚úÖ Context extracted:")
    logger.info(f"  Era: {global_context.era}")
    logger.info(f"  Lighting: {global_context.lighting_style}")
    logger.info(f"  Atmosphere: {global_context.atmosphere}")
    logger.info(f"  Characters: {len(global_context.main_characters)}")
    
    # =================================================================
    # PHASE 2: INITIAL SEGMENTATION (from GPT)
    # =================================================================
    logger.info(f"[PHASE_2] üìù Generating initial segments from narrative...")
    
    # ... (existing segment generation code) ...
    # Generate raw segments as before
    segments = await self._generate_segments_from_narrative(
        narrative, topic, style, language, 
        duration_seconds, art_style, global_context
    )
    
    # =================================================================
    # PHASE 3: STORYBOARD WITH CONSISTENCY
    # =================================================================
    logger.info(f"[PHASE_3] üé® Creating visual storyboard with frame-to-frame logic...")
    segments = await self._create_storyboard(narrative, segments, global_context)
    
    # =================================================================
    # PHASE 4: FINAL PROMPT GENERATION
    # =================================================================
    logger.info(f"[PHASE_4] üñºÔ∏è Converting storyboard to technical prompts...")
    segments = self._generate_technical_prompts(segments, global_context)
    
    # =================================================================
    # PHASE 5: CONSISTENCY CHECKS & DEDUPLICATION
    # =================================================================
    logger.info(f"[PHASE_5] ‚úì Running consistency checks...")
    segments = self._verify_frame_consistency(segments, global_context)
    segments = self.deduplicate_prompts(segments)
    
    total_duration = sum(seg.duration for seg in segments)
    logger.info(f"[VISUAL_DIRECTOR] ‚úÖ COMPLETE: {len(segments)} context-aware segments, {total_duration:.1f}s")
    
    return SegmentationResult(
        segments=segments,
        style_consistency_string=global_context.to_prompt_prefix(),
        total_duration=total_duration,
        success=True
    )
```

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ 2.5:** –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ - `_verify_frame_consistency()`

**–î–û–ë–ê–í–ò–¢–¨:**
```python
def _verify_frame_consistency(
    self,
    segments: List[VisualSegment],
    global_context: GlobalSceneContext
) -> List[VisualSegment]:
    """
    PHASE 5: Verify frame-to-frame consistency and fix issues.
    
    Checks:
    1. Does frame N+1 logically follow frame N?
    2. Are character descriptions identical?
    3. Is lighting/atmosphere consistent (unless explicit transition)?
    4. Are there sudden teleportations (location jumps without reason)?
    """
    logger.info(f"[CONSISTENCY_CHECK] Verifying {len(segments)} frames...")
    
    issues_found = 0
    fixes_applied = 0
    
    for i in range(1, len(segments)):
        prev_seg = segments[i - 1]
        curr_seg = segments[i]
        
        prev_prompt = prev_seg.visual_prompt.lower()
        curr_prompt = curr_seg.visual_prompt.lower()
        
        # Check 1: Character consistency
        for character in global_context.main_characters:
            char_desc = character["description"].lower()
            char_words = set(char_desc.split())
            
            prev_has_char = any(word in prev_prompt for word in char_words)
            curr_has_char = any(word in curr_prompt for word in char_words)
            
            if prev_has_char and curr_has_char:
                # Both frames have character - descriptions must match
                # Extract character description from prompts
                # If they differ, flag it
                # (Simplified check for now)
                pass
        
        # Check 2: Lighting consistency
        lighting_keywords = ["daylight", "night", "sunset", "sunrise", "dark", "bright"]
        prev_lighting = [kw for kw in lighting_keywords if kw in prev_prompt]
        curr_lighting = [kw for kw in lighting_keywords if kw in curr_prompt]
        
        if prev_lighting and curr_lighting and prev_lighting[0] != curr_lighting[0]:
            logger.warning(f"[CONSISTENCY] Frame {i}: Lighting changed from {prev_lighting[0]} to {curr_lighting[0]}")
            issues_found += 1
            
            # Auto-fix: Keep previous lighting
            if not curr_seg.text.lower().contains("later") and not curr_seg.text.lower().contains("next day"):
                # No temporal transition in narration, so keep same lighting
                curr_seg.visual_prompt = curr_seg.visual_prompt.replace(curr_lighting[0], prev_lighting[0])
                fixes_applied += 1
                logger.info(f"[CONSISTENCY] Auto-fixed: Changed lighting back to {prev_lighting[0]}")
        
        # Check 3: Location jumps
        # (More complex - would need location extraction)
        
    logger.info(f"[CONSISTENCY_CHECK] Issues found: {issues_found}, Auto-fixed: {fixes_applied}")
    
    return segments
```

---

## üìã –ò–¢–û–ì–û–í–´–ô –ü–õ–ê–ù (Step-by-Step)

### üîµ –≠–¢–ê–ü 1: –ê—É–¥–∏–æ-–≤–∏–¥–µ–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è (1-2 —á–∞—Å–∞)

**–§–∞–π–ª—ã:**
- `app/services/faceless_engine.py`

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**
1. ‚úÖ –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å `_calculate_segment_durations()` - —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∞—É–¥–∏–æ
2. ‚úÖ –£–±—Ä–∞—Ç—å `-t` –∏ `-shortest` –∏–∑ FFmpeg –∫–æ–º–∞–Ω–¥—ã –≤ `_render_final_video()`
3. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å pre-verification –ø–µ—Ä–µ–¥ concatenation –∫–ª–∏–ø–æ–≤
4. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ: –∞—É–¥–∏–æ vs –≤–∏–¥–µ–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

**–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**
```bash
# –°–æ–∑–¥–∞—Ç—å –≤–∏–¥–µ–æ 60 —Å–µ–∫
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–µ–∫—É–Ω–¥ - –ù–ï–¢ –ø–æ–≤—Ç–æ—Ä–∞ –∞—É–¥–∏–æ
```

---

### üîµ –≠–¢–ê–ü 2: Context-Aware Generation (3-5 —á–∞—Å–æ–≤)

**–§–∞–π–ª—ã:**
- `app/services/agents/visual_director.py`

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**
1. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å `GlobalSceneContext` dataclass
2. ‚úÖ –ù–æ–≤—ã–π –º–µ—Ç–æ–¥: `_analyze_narrative_context()` - Phase 1
3. ‚úÖ –ù–æ–≤—ã–π –º–µ—Ç–æ–¥: `_create_storyboard()` - Phase 2
4. ‚úÖ –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å `segment_story()` - 5-—Ñ–∞–∑–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å
5. ‚úÖ –ù–æ–≤—ã–π –º–µ—Ç–æ–¥: `_verify_frame_consistency()` - Phase 5
6. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è global_context

**–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**
```bash
# –°–æ–∑–¥–∞—Ç—å –≤–∏–¥–µ–æ "–ö–∞–∑–∞—Ö—Å–∫–æ–µ —Ö–∞–Ω—Å—Ç–≤–æ"
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å:
# - –≠–ø–æ—Ö–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è –≤–æ –≤—Å–µ—Ö –∫–∞–¥—Ä–∞—Ö
# - –û—Å–≤–µ—â–µ–Ω–∏–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ
# - –ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –æ–ø–∏—Å–∞–Ω—ã –∏–¥–µ–Ω—Ç–∏—á–Ω–æ
# - –ù–µ—Ç —Å–∫–∞—á–∫–æ–≤ –¥–µ–Ω—å‚Üí–Ω–æ—á—å
```

---

### üîµ –≠–¢–ê–ü 3: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã (1 —á–∞—Å)

**–°–æ–∑–¥–∞—Ç—å:**
- `CONTEXT_AWARE_GENERATION.md` - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
- –Æ–Ω–∏—Ç-—Ç–µ—Å—Ç—ã –¥–ª—è `_verify_frame_consistency()`
- –ü—Ä–∏–º–µ—Ä—ã "–¥–æ –∏ –ø–æ—Å–ª–µ"

---

## ‚ö†Ô∏è –†–ò–°–ö–ò –ò –ö–û–ú–ü–†–û–ú–ò–°–°–´

### –†–∏—Å–∫ 1: –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

**–ë—ã–ª–æ:**
- 1 API call ‚Üí 12 –ø—Ä–æ–º–ø—Ç–æ–≤ (5-10 —Å–µ–∫)

**–°—Ç–∞–Ω–µ—Ç:**
- API call 1: Context analysis (5 —Å–µ–∫)
- API call 2: Storyboard (10 —Å–µ–∫)
- API call 3: Segment generation (10 —Å–µ–∫)
- **–ò–¢–û–ì–û: +20 —Å–µ–∫—É–Ω–¥**

**–ö–æ–º–ø—Ä–æ–º–∏—Å—Å:** –ö–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ —Å–∫–æ—Ä–æ—Å—Ç–∏

### –†–∏—Å–∫ 2: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å API

**–ë—ã–ª–æ:**
- 1 √ó GPT-4o-mini call ‚âà $0.002

**–°—Ç–∞–Ω–µ—Ç:**
- 3 √ó GPT-4o-mini calls ‚âà $0.006

**–ö–æ–º–ø—Ä–æ–º–∏—Å—Å:** +$0.004 –∑–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª–æ–≤ (acceptable)

### –†–∏—Å–∫ 3: –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–ª–∞–¥–∫–∏

**–†–µ—à–µ–Ω–∏–µ:** –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π —Ñ–∞–∑—ã

---

## üéØ –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

### –î–û:
```
–ö–∞–¥—Ä 1: "Medieval warrior in foggy morning"
–ö–∞–¥—Ä 5: "Same warrior in bright daylight castle"
–ö–∞–¥—Ä 9: "Knight at night in a cave"
‚ùå –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å! –ì–¥–µ –æ–Ω? –ö–∞–∫–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫?
```

### –ü–û–°–õ–ï:
```
Global Context:
  Era: 13th century
  Time: Golden hour (sunset)
  Lighting: Warm dramatic lighting
  Atmosphere: Epic historical
  Character: "Man, 35, mongolian warrior, leather armor, brown hair, weathered face"

–ö–∞–¥—Ä 1: "Man, 35, mongolian warrior, leather armor, brown hair, weathered face, standing on hill, 13th century mongolian steppe, golden hour warm lighting, epic atmosphere, wide shot"

–ö–∞–¥—Ä 5: "Man, 35, mongolian warrior, leather armor, brown hair, weathered face, riding horse, 13th century mongolian steppe, golden hour warm lighting, epic atmosphere, medium shot"

–ö–∞–¥—Ä 9: "Man, 35, mongolian warrior, leather armor, brown hair, weathered face, leading army, 13th century mongolian steppe, golden hour warm lighting, epic atmosphere, low angle shot"

‚úÖ PERFECT CONSISTENCY!
- –ü–µ—Ä—Å–æ–Ω–∞–∂ –ò–î–ï–ù–¢–ò–ß–ï–ù
- –≠–ø–æ—Ö–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è
- –û—Å–≤–µ—â–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ
- –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞
```

---

## ü§î –í–û–ü–†–û–°–´ –î–õ–Ø –û–ë–°–£–ñ–î–ï–ù–ò–Ø

1. **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** –ù–∞—á–∞—Ç—å —Å –∞—É–¥–∏–æ-—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ (–±—ã—Å—Ç—Ä—ã–π —Ñ–∏–∫—Å) –∏–ª–∏ —Å—Ä–∞–∑—É —Å context-aware generation?

2. **API –≤—ã–∑–æ–≤—ã:** –°–æ–≥–ª–∞—Å–µ–Ω —Å 3 –≤—ã–∑–æ–≤–∞–º–∏ –≤–º–µ—Å—Ç–æ 1 (+$0.004 –∏ +20 —Å–µ–∫) —Ä–∞–¥–∏ –∫–∞—á–µ—Å—Ç–≤–∞?

3. **Fallback:** –ï—Å–ª–∏ GPT-4o-mini –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π fallback context –∏–ª–∏ –≤–µ—Ä–Ω—É—Ç—å –æ—à–∏–±–∫—É?

4. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ù—É–∂–Ω—ã –ª–∏ —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã –∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä—É—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è "–¥–æ/–ø–æ—Å–ª–µ"?

5. **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏:** –î–æ–±–∞–≤–∏—Ç—å –ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∑–∞–¥–∞–≤–∞—Ç—å global_context –≤—Ä—É—á–Ω—É—é? (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–•–æ—á—É –≤—Å–µ –∫–∞–¥—Ä—ã –≤ –Ω–æ—á–Ω–æ–º –æ—Å–≤–µ—â–µ–Ω–∏–∏")

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –î–õ–Ø –û–î–û–ë–†–ï–ù–ò–Ø

–ü–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –ø–æ–¥—Ç–≤–µ—Ä–¥–∏:

- [ ] –Ø –ø–æ–Ω—è–ª –æ–±–µ –ø—Ä–æ–±–ª–µ–º—ã (–∞—É–¥–∏–æ –ø–æ–≤—Ç–æ—Ä + —Ö–∞–æ—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã)
- [ ] –ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤—ã–≥–ª—è–¥–∏—Ç —Ä–∞–∑—É–º–Ω–æ
- [ ] –Ø —Å–æ–≥–ª–∞—Å–µ–Ω —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ API –≤—ã–∑–æ–≤–∞–º–∏ (+$0.004/–≤–∏–¥–µ–æ)
- [ ] –Ø —Å–æ–≥–ª–∞—Å–µ–Ω —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (+20 —Å–µ–∫)
- [ ] –ù–∞—á–∏–Ω–∞–µ–º —Å –∞—É–¥–∏–æ-—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ (–±—ã—Å—Ç—Ä—ã–π win)
- [ ] –ó–∞—Ç–µ–º context-aware generation (–±–æ–ª—å—à–∞—è —Ñ–∏—á–∞)

**–û—Ç–≤–µ—Ç:** –ù–∞–ø–∏—à–∏ "–û–ö" –∏–ª–∏ –∑–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å—ã!

---

**–ê–≤—Ç–æ—Ä:** Claude 4.5 Sonnet  
**–î–∞—Ç–∞:** 8 —è–Ω–≤–∞—Ä—è 2026  
**–°—Ç–∞—Ç—É—Å:** üìã –û–ñ–ò–î–ê–ù–ò–ï –û–î–û–ë–†–ï–ù–ò–Ø
